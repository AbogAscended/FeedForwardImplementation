{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:20.711321Z",
     "start_time": "2024-06-26T22:31:20.709496Z"
    }
   },
   "source": [
    "#Importing all the different python modules needed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ffModel import FeedForward"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:21.362028Z",
     "start_time": "2024-06-26T22:31:21.342736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#importing data into pandas data frame then separating the test values and input values, then finally using the DataLoader to set it up.\n",
    "trainData = pd.read_csv('train.csv')\n",
    "testValues = trainData.loc[:, 'target']\n",
    "testValuesNum = testValues.to_numpy()\n",
    "testValuesD = torch.tensor(testValuesNum, dtype=torch.float32)\n",
    "columns = trainData.columns\n",
    "columns = columns[columns != 'target']\n",
    "trainValues = trainData.loc[:, columns]\n",
    "trainValues = trainValues.drop(columns = 'id', axis=1)\n",
    "trainValuesNum = trainValues.to_numpy()\n",
    "trainValuesD = torch.tensor(trainValuesNum, dtype=torch.float32)\n",
    "trainingData = DataLoader(TensorDataset(trainValuesD, testValuesD), batch_size=25, shuffle=False)\n",
    "trainValues"
   ],
   "id": "6bf0ffa3b56b1648",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9  \\\n",
       "0   -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460 -0.443 -0.338   \n",
       "1   -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  0.557  1.202   \n",
       "2    0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317 -0.624 -0.017   \n",
       "3   -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189 -0.118 -0.638   \n",
       "4    0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566 -0.656 -0.499   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "245 -0.068 -0.184 -1.153  0.610  0.414  1.557 -0.234  0.950  0.896  1.416   \n",
       "246 -0.234 -1.373 -2.050 -0.408 -0.255  0.784  0.986 -0.891 -0.268 -0.569   \n",
       "247 -2.327 -1.834 -0.762  0.660 -0.858 -2.764 -0.539 -0.065  0.549  1.474   \n",
       "248 -0.451 -0.204 -0.762  0.261  0.022 -1.487 -1.122  0.141  0.369 -0.173   \n",
       "249  0.725  1.064  1.333 -2.863  0.203  1.898  0.434  1.207 -0.015  1.459   \n",
       "\n",
       "     ...    290    291    292    293    294    295    296    297    298    299  \n",
       "0    ...  0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n",
       "1    ... -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n",
       "2    ... -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n",
       "3    ... -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n",
       "4    ... -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "245  ...  1.492  1.430 -0.333 -0.200 -1.073  0.797  1.980  1.191  1.032 -0.402  \n",
       "246  ... -0.996  0.678  1.395  0.714  0.215 -0.537 -1.267 -1.021  0.747  0.128  \n",
       "247  ... -1.237 -0.620  0.670 -2.010  0.438  1.972 -0.379  0.676 -1.220 -0.855  \n",
       "248  ...  0.729  0.411  2.366 -0.021  0.160  0.045  0.208 -2.117 -0.546 -0.093  \n",
       "249  ... -1.028  1.081  0.607  0.550 -2.621 -0.143 -0.544 -1.690 -0.198  0.643  \n",
       "\n",
       "[250 rows x 300 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.067</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1.716</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>1.904</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.557</td>\n",
       "      <td>1.202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099</td>\n",
       "      <td>1.390</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>0.799</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>1.544</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.127</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.805</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.811</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.442</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.557</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1.416</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492</td>\n",
       "      <td>1.430</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.797</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.234</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-2.050</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-1.267</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-2.327</td>\n",
       "      <td>-1.834</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-2.764</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.549</td>\n",
       "      <td>1.474</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-2.010</td>\n",
       "      <td>0.438</td>\n",
       "      <td>1.972</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>-0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2.366</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-2.117</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.725</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.333</td>\n",
       "      <td>-2.863</td>\n",
       "      <td>0.203</td>\n",
       "      <td>1.898</td>\n",
       "      <td>0.434</td>\n",
       "      <td>1.207</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>1.459</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-2.621</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 300 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:22.654660Z",
     "start_time": "2024-06-26T22:31:22.652077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Now its time to set up the model, optimizer, and the loss function to actually preform the training.\n",
    "For the loss equation as im doing uni-variate normal regression to determine the correct numerical value based on 8 input values.\n",
    "I will at first use default hyperparameters and assess how well the model is doing then adjust from there.\n",
    "'''\n",
    "model = FeedForward().to('cuda')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss = nn.MSELoss()"
   ],
   "id": "369a30afb467ad28",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:23.353159Z",
     "start_time": "2024-06-26T22:31:23.350709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Now I will implement a modular training loop\n",
    "'''\n",
    "def trainLoop(dataloader, model, lossf, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        y = y.view(-1, 1)\n",
    "\n",
    "        # Forward pass through NN\n",
    "        prediction = model(x)\n",
    "        loss = lossf(prediction, y)\n",
    "\n",
    "        # Backwards pass through NN\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss_value = loss.item()\n",
    "            current = batch * dataloader.batch_size + len(x)\n",
    "            print(f\"Batch: {batch}, Batch Size: {len(x)}\")\n",
    "            print(f\"Loss: {loss_value:>7f} [{current}/{size}]\")"
   ],
   "id": "89a68e32e05b9133",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:24.267090Z",
     "start_time": "2024-06-26T22:31:24.264926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_loop(dataloader, model, lossf):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            y = y.view(-1, 1)\n",
    "            pred = model(X)\n",
    "            total_loss += lossf(pred, y).item() * X.size(0)\n",
    "\n",
    "    avg_loss = total_loss / size\n",
    "    print(f\"Test Error: \\n Avg loss: {avg_loss:>8f} \\n\")"
   ],
   "id": "8d89331bc4013f58",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:39.038489Z",
     "start_time": "2024-06-26T22:31:39.012925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Now to actually train the model in 10 epochs\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch + 1}\\n ------------------------------------------------')\n",
    "    trainLoop(trainingData, model, loss, optimizer)\n",
    "    test_loop(trainingData, model, loss)\n",
    "print('Done!')"
   ],
   "id": "1e526d3dad4bbab0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ------------------------------------------------\n",
      "Batch: 0, Batch Size: 25\n",
      "Loss: 0.020498 [25/250]\n",
      "Batch: 5, Batch Size: 25\n",
      "Loss: 0.033251 [150/250]\n",
      "Test Error: \n",
      " Avg loss: 0.023665 \n",
      "\n",
      "Epoch 2\n",
      " ------------------------------------------------\n",
      "Batch: 0, Batch Size: 25\n",
      "Loss: 0.005515 [25/250]\n",
      "Batch: 5, Batch Size: 25\n",
      "Loss: 0.011402 [150/250]\n",
      "Test Error: \n",
      " Avg loss: 0.009457 \n",
      "\n",
      "Epoch 3\n",
      " ------------------------------------------------\n",
      "Batch: 0, Batch Size: 25\n",
      "Loss: 0.004931 [25/250]\n",
      "Batch: 5, Batch Size: 25\n",
      "Loss: 0.003363 [150/250]\n",
      "Test Error: \n",
      " Avg loss: 0.003322 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:31:28.766323Z",
     "start_time": "2024-06-26T22:31:26.651055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#now to set up the testing to submit to kaggle.\n",
    "testData = pd.read_csv('test.csv')\n",
    "savedID = testData.loc[:, 'id']\n",
    "testDataD = testData.drop(columns = 'id')\n",
    "testDataD = testDataD.to_numpy()\n",
    "testDataD = torch.tensor(testDataD, dtype=torch.float32)\n",
    "testValuesFinal = DataLoader(TensorDataset(testDataD), shuffle=False)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for item in testValuesFinal:\n",
    "        item = item[0].to('cuda')\n",
    "        prediction = model.forward(item)\n",
    "        predictions.append(prediction.cpu().numpy())\n",
    "submission = np.concatenate(predictions).reshape(-1, 1)\n",
    "submission = pd.DataFrame(submission, columns=['target'])\n",
    "final = pd.concat([savedID, submission], axis=1)\n",
    "final.to_csv('submission.csv', index=False)"
   ],
   "id": "c7b5d1c798aa032",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T22:30:52.359994Z",
     "start_time": "2024-06-26T22:30:52.358928Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3d3cb077b50889e2",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
